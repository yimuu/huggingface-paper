import asyncio
import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import date
from typing import List, Optional

import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query
from fastapi_cache import FastAPICache
from fastapi_cache.backends.inmemory import InMemoryBackend
from fastapi_cache.decorator import cache
from pydantic import BaseModel, HttpUrl
from openai import AsyncOpenAI, OpenAIError

# --- 1. é…ç½® ---
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è·å–ç¼“å­˜æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º12å°æ—¶
CACHE_EXPIRE_SECONDS = int(os.getenv("CACHE_EXPIRE_SECONDS", 43200))

# --- OpenAI é…ç½® ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL_NAME = os.getenv("OPENAI_MODEL_NAME")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")

# å…¨å±€ OpenAI å®¢æˆ·ç«¯
openai_client: Optional[AsyncOpenAI] = None

# --- LLM Prompt Template ---
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åé¡¶çº§çš„AIç ”ç©¶å‘˜åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºè®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œæä¾›ä¸€ä»½ç®€æ´è€Œæ·±åˆ»çš„ä¸­æ–‡è§£è¯»ã€‚

**è®ºæ–‡æ ‡é¢˜ï¼š** {title}
**è®ºæ–‡æ‘˜è¦ï¼š** {summary}

è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†çš„Markdownæ ¼å¼è§£è¯»ï¼š
1.  **æ ¸å¿ƒè´¡çŒ® (Contribution):** æ¸…æ™°åœ°é˜è¿°è¿™ç¯‡è®ºæ–‡æœ€ä¸»è¦çš„åˆ›æ–°ç‚¹æˆ–å‘ç°ã€‚å®ƒè§£å†³äº†ä»€ä¹ˆæ–°é—®é¢˜ï¼Œæˆ–è€…æå‡ºäº†ä»€ä¹ˆæ–°æ–¹æ³•ï¼Ÿ
2.  **æ–¹æ³•ç®€ä»‹ (Method):** ç®€è¦ä»‹ç»è®ºæ–‡ä¸­ä½¿ç”¨çš„æ ¸å¿ƒæ–¹æ³•ã€æ¨¡å‹æ¶æ„æˆ–æŠ€æœ¯è·¯å¾„ã€‚è¯·å°½é‡ä½¿ç”¨æ˜“äºç†è§£çš„è¯­è¨€ï¼Œé¿å…ä¸å¿…è¦çš„æœ¯è¯­ã€‚

è¯·ç¡®ä¿ä½ çš„è§£è¯»å†…å®¹å®Œå…¨åŸºäºæ‰€æä¾›çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œå¹¶ä½¿ç”¨æµç•…çš„ä¸­æ–‡è¿›è¡Œè¡¨è¿°ã€‚
"""


# --- 2. Pydantic æ•°æ®æ¨¡å‹ ---
class Paper(BaseModel):
    """ä»£è¡¨ä¸€ç¯‡è®ºæ–‡ä¿¡æ¯çš„æ¨¡å‹"""
    title: str
    summary: str
    arxiv_url: HttpUrl
    huggingface_url: HttpUrl
    authors: List[str]
    github_url: Optional[HttpUrl] = None
    upvotes: int = 0
    ai_summary: Optional[str] = None
    ai_keywords: List[str] = []

class PaperList(BaseModel):
    """ç”¨äºæ¥æ”¶è®ºæ–‡åˆ—è¡¨çš„è¯·æ±‚ä½“æ¨¡å‹"""
    papers: List[Paper]

class MarkdownResponse(BaseModel):
    """ç”¨äºè¿”å›MarkdownæŠ¥å‘Šçš„å“åº”ä½“æ¨¡å‹"""
    markdown_report: str


# --- 3. FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† (lifespan) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global openai_client
    logger.info("Application is starting up...")
    
    # åˆå§‹åŒ–ç¼“å­˜
    FastAPICache.init(InMemoryBackend(), prefix="fastapi-cache")
    logger.info(f"Cache initialized. Expire time: {CACHE_EXPIRE_SECONDS} seconds.")

    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    if not OPENAI_API_KEY:
        logger.warning("OPENAI_API_KEY not found. The report generation endpoint will not work.")
    else:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
        logger.info(f"OpenAI client initialized for model: {OPENAI_MODEL_NAME}")
    
    yield
    
    logger.info("Application is shutting down...")
    if openai_client:
        await openai_client.close()


# --- 4. FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="AIæ¯æ—¥è®ºæ–‡è§£è¯»æœåŠ¡",
    description="ä¸€ä¸ªAPIæœåŠ¡ï¼Œç”¨äºä¸€é”®çˆ¬å–Hugging Faceæ¯æ—¥è®ºæ–‡å¹¶ç”ŸæˆLLMä¸­æ–‡è§£è¯»æŠ¥å‘Šã€‚",
    version="4.0.0-final",
    lifespan=lifespan
)

# --- 5. æ ¸å¿ƒé€»è¾‘å‡½æ•° (è§£è€¦&å¯å¤ç”¨) ---

# å°†ç¼“å­˜è£…é¥°å™¨åº”ç”¨åœ¨æ ¸å¿ƒçˆ¬è™«å‡½æ•°ä¸Š
@cache(expire=CACHE_EXPIRE_SECONDS)
async def scrape_papers_for_date(date_str: str) -> List[Paper]:
    """
    çˆ¬å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡æ•°æ®ã€‚æ­¤å‡½æ•°çš„ç»“æœå°†è¢«ç¼“å­˜ã€‚
    """
    url = f"https://huggingface.co/papers/date/{date_str}"
    logger.info(f"ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¸ºæ—¥æœŸ {date_str} å®æ—¶çˆ¬å–è®ºæ–‡: {url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36", 
        "Accept-Language": "en-US,en;q=0.9"
    }
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers=headers, timeout=20.0, follow_redirects=True)
            response.raise_for_status()
    except httpx.RequestError as exc:
        logger.error(f"è¯·æ±‚Hugging FaceæœåŠ¡å™¨å¤±è´¥: {exc}")
        raise HTTPException(status_code=503, detail=f"è¯·æ±‚Hugging FaceæœåŠ¡å™¨å¤±è´¥: {exc}")
    except httpx.HTTPStatusError as exc:
        logger.error(f"Hugging FaceæœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ {exc.response.status_code} for URL {url}")
        if exc.response.status_code == 404:
            # å¯¹äºçˆ¬è™«å‡½æ•°ï¼Œè¿”å›ç©ºåˆ—è¡¨æ¯”æŠ›å‡ºHTTPå¼‚å¸¸æ›´é€šç”¨
            return []
        raise HTTPException(status_code=exc.response.status_code, detail=f"Hugging FaceæœåŠ¡å™¨è¿”å›é”™è¯¯: {exc.response.text}")

    logger.info(f"æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œå¼€å§‹è§£æHTML...")

    soup = BeautifulSoup(response.text, 'lxml')
    data_div = soup.find('div', attrs={'data-target': 'DailyPapers'})
    if not data_div or 'data-props' not in data_div.attrs:
        logger.warning(f"åœ¨é¡µé¢ {url} ä¸Šæœªæ‰¾åˆ° 'DailyPapers' æ•°æ®å—ã€‚")
        return []
        
    try:
        props_data = json.loads(data_div['data-props'])
        papers_list_json = props_data.get('dailyPapers', [])
    except (json.JSONDecodeError, KeyError) as e:
        logger.exception("è§£æé¡µé¢å†…åµŒJSONæ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ã€‚")
        raise HTTPException(status_code=500, detail=f"è§£æé¡µé¢å†…åµŒJSONæ•°æ®å¤±è´¥: {e}")

    results: List[Paper] = []
    for item in papers_list_json:
        paper_data = item.get('paper', {})
        paper_id = paper_data.get('id')
        title = paper_data.get('title')
        if not paper_id or not title:
            logger.warning(f"è·³è¿‡ä¸€æ¡ä¸å®Œæ•´çš„è®ºæ–‡è®°å½•: {item}")
            continue
        try:
            cleaned_title = ' '.join(title.split())
            cleaned_summary = ' '.join(paper_data.get('summary', '').split())

            paper_obj = Paper(
                title=cleaned_title,
                summary=cleaned_summary,
                arxiv_url=f"https://arxiv.org/abs/{paper_id}",
                huggingface_url=f"https://huggingface.co/papers/{paper_id}",
                authors=[author.get("name") for author in paper_data.get("authors", []) if author.get("name")],
                github_url=paper_data.get("githubRepo"),
                upvotes=paper_data.get("upvotes", 0),
                ai_summary=paper_data.get("ai_summary"),
                ai_keywords=paper_data.get("ai_keywords", [])
            )
            results.append(paper_obj)
        except Exception as e:
            logger.error(f"å¤„ç†è®ºæ–‡ {paper_id} æ—¶å‡ºé”™: {e}. Raw data: {paper_data}")
            continue
    
    logger.info(f"æˆåŠŸè§£æå¹¶å¤„ç†äº† {len(results)} ç¯‡è®ºæ–‡ï¼ˆæ—¥æœŸ: {date_str}ï¼‰ã€‚")
    return results


async def interpret_single_paper(paper: Paper) -> str:
    """ä½¿ç”¨LLMè§£è¯»å•ç¯‡è®ºæ–‡"""
    if not openai_client: return "é”™è¯¯ï¼šOpenAIå®¢æˆ·ç«¯æœªé…ç½®ã€‚"
    prompt = PROMPT_TEMPLATE.format(title=paper.title, summary=paper.summary)
    try:
        response = await openai_client.chat.completions.create(
            model=OPENAI_MODEL_NAME, 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3, 
            max_tokens=8000)
        return response.choices[0].message.content or paper.summary
    except OpenAIError as e:
        logger.error(f"è°ƒç”¨OpenAI APIå¤±è´¥ï¼Œè®ºæ–‡æ ‡é¢˜: '{paper.title}': {e}")
        return f"é”™è¯¯ï¼šæ— æ³•ä»LLMè·å–è§£è¯»ç»“æœã€‚è¯¦æƒ…: {e}"
    except Exception as e:
        logger.error(f"è§£è¯»è®ºæ–‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè®ºæ–‡æ ‡é¢˜: '{paper.title}': {e}")
        return "é”™è¯¯ï¼šå‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚"

# --- 6. API ç«¯ç‚¹ ---
@app.get(
    "/daily_report",
    response_model=MarkdownResponse,
    summary="ä¸€é”®è·å–å¹¶è§£è¯»æ¯æ—¥çƒ­é—¨è®ºæ–‡"
)
async def get_daily_report(
    request_date: Optional[str] = Query(None, description="è¦è·å–æŠ¥å‘Šçš„æ—¥æœŸ (YYYY-MM-DD)ï¼Œç•™ç©ºåˆ™ä¸ºä»Šå¤©ã€‚"),
    max_papers: int = Query(3, ge=1, le=10, description="è§£è¯»è®ºæ–‡çš„æœ€å¤§æ•°é‡ï¼ŒæŒ‰ç‚¹èµæ•°æ’åºã€‚")
):
    """
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¸­æ–‡è§£è¯»ï¼Œå¹¶è¿”å›ä¸€ä»½å®Œæ•´çš„MarkdownæŠ¥å‘Šã€‚
    """
    if not openai_client:
        raise HTTPException(status_code=503, detail="OpenAIå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•æä¾›è§£è¯»æœåŠ¡ã€‚")

    target_date = date.today()
    if request_date:
        try:
            target_date = date.fromisoformat(request_date)
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ 'YYYY-MM-DD' æ ¼å¼ã€‚")
    date_str = target_date.isoformat()

    # 1. è·å–è®ºæ–‡æ•°æ® (æ­¤æ­¥éª¤ä¼šè¢«ç¼“å­˜)
    papers = await scrape_papers_for_date(date_str)
    if not papers:
        return MarkdownResponse(markdown_report=f"# AIæ¯æ—¥è®ºæ–‡è§£è¯» ({date_str})\n\nä»Šå¤©æ²¡æœ‰æ‰¾åˆ°å¯ä»¥è§£è¯»çš„è®ºæ–‡ã€‚")

    # 2. æ’åºå’Œç­›é€‰
    sorted_papers = sorted(papers, key=lambda p: p.upvotes, reverse=True)
    papers_to_process = sorted_papers[:max_papers]
    logger.info(f"å…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œå°†è§£è¯»çƒ­åº¦æœ€é«˜çš„ {len(papers_to_process)} ç¯‡ã€‚")
    
    # 3. å¹¶å‘è°ƒç”¨LLMè§£è¯»
    tasks = [interpret_single_paper(p) for p in papers_to_process]
    interpretations = await asyncio.gather(*tasks)

    # 4. æ„å»ºMarkdownæŠ¥å‘Š
    report_parts = [f"# AIæ¯æ—¥è®ºæ–‡è§£è¯» ({date_str})\n"]
    report_parts.append(f"ç²¾é€‰Hugging Faceä¸Šç‚¹èµæ•°æœ€é«˜çš„ {len(papers_to_process)} ç¯‡è®ºæ–‡è¿›è¡Œè§£è¯»ã€‚\n---")
    for i, paper in enumerate(papers_to_process):
        report_parts.append(f"\n## {i+1}. {paper.title}\n")
        report_parts.append(f"**ç‚¹èµæ•°:** {paper.upvotes} | **ä½œè€…:** {', '.join(paper.authors)}\n")
        report_parts.append(f"> [arXivé“¾æ¥]({paper.arxiv_url}) | [Hugging Faceé“¾æ¥]({paper.huggingface_url})\n")
        report_parts.append("### ğŸ¤– LLM è§£è¯»\n")
        report_parts.append(interpretations[i])
        report_parts.append("\n---")
    
    return MarkdownResponse(markdown_report="\n".join(report_parts))

@app.get(
    "/papers",
    response_model=List[Paper],
    summary="ä»…è·å–æŒ‡å®šæ—¥æœŸçš„åŸå§‹è®ºæ–‡æ•°æ®",
    description="ä¸€ä¸ªè¾…åŠ©æ¥å£ï¼Œç”¨äºè·å–åŸå§‹ã€æœªåŠ å·¥çš„è®ºæ–‡æ•°æ®åˆ—è¡¨ã€‚"
)
async def get_raw_papers(request_date: Optional[str] = Query(None, description="æ—¥æœŸ (YYYY-MM-DD)ï¼Œç•™ç©ºä¸ºä»Šå¤©ã€‚")):
    target_date = date.today()
    if request_date:
        try:
            target_date = date.fromisoformat(request_date)
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ 'YYYY-MM-DD' æ ¼å¼ã€‚")
    return await scrape_papers_for_date(target_date.isoformat())

# --- 7. ç”¨äºæœ¬åœ°è¿è¡Œçš„å…¥å£ ---
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)